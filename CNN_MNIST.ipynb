{
    "cells": [
        {
            "metadata": {},
            "id": "223a24a4-a370-4b00-8470-36c037308173",
            "cell_type": "code",
            "source": "# Conv.py takes input number of filters and its function forward takes the image array as input and \n# overall returns 3d array with filters applied,\n# input : 8   28*28 array\n# Output: 26*26*8 array\nimport os\nos.system('pip install mnist')\nimport numpy as np\nclass conv3x3:\n    def __init__(self,num_filters):\n        self.num_filters=num_filters\n        self.filters=np.random.rand(num_filters,3,3)\n\n    def iterate_regions(self,image):\n        d1,d2=image.shape\n        for i in range(d1-2):\n            for j in range(d2-2):\n                region=image[i:(i+3),j:(j+3)]\n                yield region,i,j\n\n    def forward(self,input):\n        d1,d2=input.shape\n        self.last_input=input\n        output=np.zeros((d1-2,d2-2,self.num_filters))\n        for region,i,j in self.iterate_regions(input):\n            output[i,j]=np.sum(region*self.filters,axis=(1,2))\n        return output\n    \n    def backprop(self, d_L_d_out, learn_rate):\n\n        d_L_d_filters = np.zeros(self.filters.shape)\n\n        for im_region, i, j in self.iterate_regions(self.last_input):\n            for f in range(self.num_filters):\n                d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n\n        # Update filters\n        self.filters -= learn_rate * d_L_d_filters\n\n        return None",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "id": "39666f49-0d48-426b-bd09-84b901dd2c57",
            "cell_type": "code",
            "source": "#Maxpool's function forward takes 26*26*8 input and do maxpool and return 13*13*8 array.\nimport numpy as np\nclass max_pool:\n    def iterate_regions(self,image):\n        d1,d2,d3=image.shape\n        for i in range(d1//2):\n            for j in range(d2//2):\n                region=image[2*i:2*(i+1),2*j:2*(j+1)]\n                yield region,i,j\n    def forward(self,input):\n        d1,d2,d3=input.shape\n        self.last_input = input\n        output=np.zeros((d1//2,d2//2,d3))\n        for region,i,j in self.iterate_regions(input):\n            output[i,j]=np.max(region,axis=(0,1))\n        return output\n    def backprop(self, d_L_d_out):\n        d_L_d_input = np.zeros(self.last_input.shape)\n\n        for im_region, i, j in self.iterate_regions(self.last_input):\n            h, w, f = im_region.shape\n            amax = np.amax(im_region, axis=(0, 1))\n\n        for i2 in range(h):\n            for j2 in range(w):\n                for f2 in range(f):\n                    # If this pixel was the max value, copy the gradient to it.\n                    if im_region[i2, j2, f2] == amax[f2]:\n                        d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n\n        return d_L_d_input",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {},
            "id": "73deafc8-fb3c-4bbb-af94-f0ed3ab481fc",
            "cell_type": "code",
            "source": "# This class soft_max takes input of (number of elements in new maxpooled img \"13*13*8=1352\") \n# and (number of nodes 10) and forward function takes input of the img array\n# and returns a array 10 with probablities for each one.\nimport numpy as np\nclass soft_max:\n    def __init__(self,input_len,num_nodes):\n        self.weights=np.random.randn(input_len,num_nodes)/input_len\n        self.biases=np.zeros(num_nodes)\n    def forward(self,input):\n        self.last_input_shape = input.shape\n        input=input.flatten()\n        self.last_input = input\n        #calculaing wX+b\n        totals=np.dot(input,self.weights)+self.biases\n        self.last_totals=totals\n        #final output\n        expx=np.exp(totals)\n        output=expx/np.sum(expx,axis=0)\n        return output\n    def backprop(self,d_L_d_out,learn_rate):\n        for i, gradient in enumerate(d_L_d_out):\n            if gradient == 0:\n                continue\n\n            # e^totals\n            t_exp = np.exp(self.last_totals)\n\n            # Sum of all e^totals\n            S = np.sum(t_exp)\n\n            # Gradients of out[i] against totals\n            d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n            d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n\n            # Gradients of totals against weights/biases/input\n            d_t_d_w = self.last_input\n            d_t_d_b = 1\n            d_t_d_inputs = self.weights\n\n            # Gradients of loss against totals\n            d_L_d_t = gradient * d_out_d_t\n\n            # Gradients of loss against weights/biases/input\n            d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n            d_L_d_b = d_L_d_t * d_t_d_b\n            d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n\n            # Update weights / biases\n            self.weights -= learn_rate * d_L_d_w\n            self.biases -= learn_rate * d_L_d_b\n            return d_L_d_inputs.reshape(self.last_input_shape)\n",
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {},
            "id": "327e01e7-3090-4db7-8f72-5487ec437665",
            "cell_type": "code",
            "source": "import mnist\nimport numpy as np\n\nconv=conv3x3(8)\npool=max_pool()\nSoftmax=soft_max(13*13*8,10)\n\ntrain_images=mnist.train_images()[:10000]\ntrain_labels=mnist.train_labels()[:10000]\ntest_images = mnist.test_images()[:1000]\ntest_labels = mnist.test_labels()[:1000]\n\ndef forward(image,label):\n    output=conv.forward((image/255)-0.5)\n    output=pool.forward(output)\n    output=Softmax.forward(output)\n\n    loss=-np.log(output[label])\n    accuracy=1 if np.argmax(output)==label else 0\n\n    return output, loss, accuracy\n\ndef train(im, label, lr=.005):\n        # Forward\n    out, loss, acc = forward(im, label)\n\n    gradient = np.zeros(10)\n    gradient[label] = -1 / out[label]\n\n    # Backprop\n    gradient = soft_max.backprop(Softmax,gradient, lr)\n    gradient = pool.backprop(gradient)\n    gradient = conv.backprop(gradient, lr)\n\n    return loss, acc\n\n\nprint('CNN initialised')\n\n# Training the CNN for 3 epochs\nfor epoch in range(3):\n    print('--- Epoch %d ---' % (epoch + 1))\n\n\n    permutation = np.random.permutation(len(train_images))\n    train_images = train_images[permutation]\n    train_labels = train_labels[permutation]\n\n    loss = 0\n    num_correct = 0\n    for i, (im, label) in enumerate(zip(train_images, train_labels)):\n        if i > 0 and i % 100 == 99:\n            print(\n                '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n                (i + 1, loss / 100, num_correct)\n            )\n            loss = 0\n            num_correct = 0\n\n        l, acc = train(im, label)\n        loss += l\n        num_correct += acc\n\n\nprint('\\n--- Testing the CNN ---')\nloss = 0\nnum_correct = 0\nfor im, label in zip(test_images, test_labels):\n    _, l, acc = forward(im, label)\n    loss += l\n    num_correct += acc\n\nnum_tests = len(test_images)\nprint('Test Loss:', loss / num_tests)\nprint('Test Accuracy:', num_correct / num_tests)",
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "CNN initialised\n--- Epoch 1 ---\n[Step 100] Past 100 steps: Average Loss 10.824 | Accuracy: 28%\n[Step 200] Past 100 steps: Average Loss 6.215 | Accuracy: 51%\n[Step 300] Past 100 steps: Average Loss 5.401 | Accuracy: 59%\n[Step 400] Past 100 steps: Average Loss 4.722 | Accuracy: 62%\n[Step 500] Past 100 steps: Average Loss 3.966 | Accuracy: 66%\n[Step 600] Past 100 steps: Average Loss 4.556 | Accuracy: 68%\n[Step 700] Past 100 steps: Average Loss 4.575 | Accuracy: 66%\n[Step 800] Past 100 steps: Average Loss 3.140 | Accuracy: 72%\n[Step 900] Past 100 steps: Average Loss 3.091 | Accuracy: 75%\n[Step 1000] Past 100 steps: Average Loss 3.236 | Accuracy: 78%\n[Step 1100] Past 100 steps: Average Loss 3.638 | Accuracy: 74%\n[Step 1200] Past 100 steps: Average Loss 2.873 | Accuracy: 75%\n[Step 1300] Past 100 steps: Average Loss 3.004 | Accuracy: 76%\n[Step 1400] Past 100 steps: Average Loss 3.670 | Accuracy: 69%\n[Step 1500] Past 100 steps: Average Loss 2.332 | Accuracy: 78%\n[Step 1600] Past 100 steps: Average Loss 3.176 | Accuracy: 77%\n[Step 1700] Past 100 steps: Average Loss 2.432 | Accuracy: 77%\n[Step 1800] Past 100 steps: Average Loss 2.940 | Accuracy: 83%\n[Step 1900] Past 100 steps: Average Loss 2.670 | Accuracy: 74%\n[Step 2000] Past 100 steps: Average Loss 2.350 | Accuracy: 83%\n[Step 2100] Past 100 steps: Average Loss 4.409 | Accuracy: 70%\n[Step 2200] Past 100 steps: Average Loss 3.779 | Accuracy: 71%\n[Step 2300] Past 100 steps: Average Loss 2.412 | Accuracy: 79%\n[Step 2400] Past 100 steps: Average Loss 2.236 | Accuracy: 82%\n[Step 2500] Past 100 steps: Average Loss 1.953 | Accuracy: 84%\n[Step 2600] Past 100 steps: Average Loss 1.910 | Accuracy: 83%\n[Step 2700] Past 100 steps: Average Loss 2.141 | Accuracy: 74%\n[Step 2800] Past 100 steps: Average Loss 3.208 | Accuracy: 81%\n[Step 2900] Past 100 steps: Average Loss 2.529 | Accuracy: 75%\n[Step 3000] Past 100 steps: Average Loss 1.162 | Accuracy: 86%\n[Step 3100] Past 100 steps: Average Loss 3.236 | Accuracy: 78%\n[Step 3200] Past 100 steps: Average Loss 1.466 | Accuracy: 86%\n[Step 3300] Past 100 steps: Average Loss 3.978 | Accuracy: 74%\n[Step 3400] Past 100 steps: Average Loss 2.150 | Accuracy: 80%\n[Step 3500] Past 100 steps: Average Loss 2.305 | Accuracy: 85%\n[Step 3600] Past 100 steps: Average Loss 2.176 | Accuracy: 84%\n[Step 3700] Past 100 steps: Average Loss 2.169 | Accuracy: 85%\n[Step 3800] Past 100 steps: Average Loss 3.534 | Accuracy: 79%\n[Step 3900] Past 100 steps: Average Loss 2.054 | Accuracy: 84%\n[Step 4000] Past 100 steps: Average Loss 1.472 | Accuracy: 89%\n[Step 4100] Past 100 steps: Average Loss 4.157 | Accuracy: 74%\n[Step 4200] Past 100 steps: Average Loss 2.251 | Accuracy: 85%\n[Step 4300] Past 100 steps: Average Loss 2.495 | Accuracy: 81%\n[Step 4400] Past 100 steps: Average Loss 2.346 | Accuracy: 82%\n[Step 4500] Past 100 steps: Average Loss 1.951 | Accuracy: 76%\n[Step 4600] Past 100 steps: Average Loss 1.231 | Accuracy: 89%\n[Step 4700] Past 100 steps: Average Loss 1.766 | Accuracy: 80%\n[Step 4800] Past 100 steps: Average Loss 1.911 | Accuracy: 78%\n[Step 4900] Past 100 steps: Average Loss 2.404 | Accuracy: 83%\n[Step 5000] Past 100 steps: Average Loss 2.496 | Accuracy: 81%\n[Step 5100] Past 100 steps: Average Loss 1.879 | Accuracy: 82%\n[Step 5200] Past 100 steps: Average Loss 1.999 | Accuracy: 83%\n[Step 5300] Past 100 steps: Average Loss 3.139 | Accuracy: 81%\n[Step 5400] Past 100 steps: Average Loss 2.679 | Accuracy: 80%\n[Step 5500] Past 100 steps: Average Loss 2.964 | Accuracy: 79%\n[Step 5600] Past 100 steps: Average Loss 1.934 | Accuracy: 85%\n[Step 5700] Past 100 steps: Average Loss 3.256 | Accuracy: 75%\n[Step 5800] Past 100 steps: Average Loss 1.910 | Accuracy: 81%\n[Step 5900] Past 100 steps: Average Loss 2.145 | Accuracy: 82%\n[Step 6000] Past 100 steps: Average Loss 3.174 | Accuracy: 76%\n[Step 6100] Past 100 steps: Average Loss 0.902 | Accuracy: 86%\n[Step 6200] Past 100 steps: Average Loss 2.943 | Accuracy: 74%\n[Step 6300] Past 100 steps: Average Loss 2.004 | Accuracy: 83%\n[Step 6400] Past 100 steps: Average Loss 3.067 | Accuracy: 77%\n[Step 6500] Past 100 steps: Average Loss 1.338 | Accuracy: 88%\n[Step 6600] Past 100 steps: Average Loss 1.356 | Accuracy: 84%\n[Step 6700] Past 100 steps: Average Loss 1.642 | Accuracy: 86%\n[Step 6800] Past 100 steps: Average Loss 1.950 | Accuracy: 85%\n[Step 6900] Past 100 steps: Average Loss 1.308 | Accuracy: 86%\n[Step 7000] Past 100 steps: Average Loss 0.517 | Accuracy: 90%\n[Step 7100] Past 100 steps: Average Loss 1.722 | Accuracy: 85%\n[Step 7200] Past 100 steps: Average Loss 1.998 | Accuracy: 77%\n[Step 7300] Past 100 steps: Average Loss 0.727 | Accuracy: 91%\n[Step 7400] Past 100 steps: Average Loss 1.421 | Accuracy: 88%\n[Step 7500] Past 100 steps: Average Loss 1.909 | Accuracy: 84%\n[Step 7600] Past 100 steps: Average Loss 2.028 | Accuracy: 84%\n[Step 7700] Past 100 steps: Average Loss 0.900 | Accuracy: 85%\n[Step 7800] Past 100 steps: Average Loss 1.473 | Accuracy: 81%\n[Step 7900] Past 100 steps: Average Loss 3.210 | Accuracy: 77%\n[Step 8000] Past 100 steps: Average Loss 0.467 | Accuracy: 94%\n[Step 8100] Past 100 steps: Average Loss 2.777 | Accuracy: 83%\n[Step 8200] Past 100 steps: Average Loss 1.291 | Accuracy: 88%\n[Step 8300] Past 100 steps: Average Loss 2.109 | Accuracy: 83%\n[Step 8400] Past 100 steps: Average Loss 0.901 | Accuracy: 87%\n[Step 8500] Past 100 steps: Average Loss 0.487 | Accuracy: 89%\n[Step 8600] Past 100 steps: Average Loss 0.821 | Accuracy: 91%\n[Step 8700] Past 100 steps: Average Loss 1.963 | Accuracy: 82%\n[Step 8800] Past 100 steps: Average Loss 1.457 | Accuracy: 87%\n[Step 8900] Past 100 steps: Average Loss 2.199 | Accuracy: 77%\n[Step 9000] Past 100 steps: Average Loss 1.066 | Accuracy: 89%\n[Step 9100] Past 100 steps: Average Loss 1.485 | Accuracy: 81%\n[Step 9200] Past 100 steps: Average Loss 2.845 | Accuracy: 84%\n[Step 9300] Past 100 steps: Average Loss 2.177 | Accuracy: 83%\n[Step 9400] Past 100 steps: Average Loss 0.969 | Accuracy: 93%\n[Step 9500] Past 100 steps: Average Loss 1.788 | Accuracy: 82%\n[Step 9600] Past 100 steps: Average Loss 1.912 | Accuracy: 80%\n[Step 9700] Past 100 steps: Average Loss 1.659 | Accuracy: 86%\n[Step 9800] Past 100 steps: Average Loss 1.832 | Accuracy: 85%\n[Step 9900] Past 100 steps: Average Loss 1.292 | Accuracy: 82%\n[Step 10000] Past 100 steps: Average Loss 2.307 | Accuracy: 83%\n--- Epoch 2 ---\n[Step 100] Past 100 steps: Average Loss 1.082 | Accuracy: 86%\n[Step 200] Past 100 steps: Average Loss 1.132 | Accuracy: 91%\n[Step 300] Past 100 steps: Average Loss 0.725 | Accuracy: 88%\n[Step 400] Past 100 steps: Average Loss 0.642 | Accuracy: 90%\n[Step 500] Past 100 steps: Average Loss 0.756 | Accuracy: 92%\n[Step 600] Past 100 steps: Average Loss 1.407 | Accuracy: 87%\n[Step 700] Past 100 steps: Average Loss 1.477 | Accuracy: 89%\n[Step 800] Past 100 steps: Average Loss 1.172 | Accuracy: 86%\n[Step 900] Past 100 steps: Average Loss 1.016 | Accuracy: 89%\n[Step 1000] Past 100 steps: Average Loss 2.535 | Accuracy: 80%\n[Step 1100] Past 100 steps: Average Loss 0.687 | Accuracy: 89%\n[Step 1200] Past 100 steps: Average Loss 0.783 | Accuracy: 90%\n[Step 1300] Past 100 steps: Average Loss 1.290 | Accuracy: 88%\n[Step 1400] Past 100 steps: Average Loss 1.442 | Accuracy: 85%\n[Step 1500] Past 100 steps: Average Loss 0.663 | Accuracy: 87%\n[Step 1600] Past 100 steps: Average Loss 1.169 | Accuracy: 89%\n[Step 1700] Past 100 steps: Average Loss 2.063 | Accuracy: 84%\n[Step 1800] Past 100 steps: Average Loss 0.791 | Accuracy: 90%\n[Step 1900] Past 100 steps: Average Loss 1.300 | Accuracy: 84%\n[Step 2000] Past 100 steps: Average Loss 0.609 | Accuracy: 92%\n[Step 2100] Past 100 steps: Average Loss 0.828 | Accuracy: 90%\n[Step 2200] Past 100 steps: Average Loss 1.403 | Accuracy: 86%\n[Step 2300] Past 100 steps: Average Loss 1.075 | Accuracy: 82%\n[Step 2400] Past 100 steps: Average Loss 0.837 | Accuracy: 89%\n[Step 2500] Past 100 steps: Average Loss 0.314 | Accuracy: 93%\n[Step 2600] Past 100 steps: Average Loss 1.627 | Accuracy: 82%\n[Step 2700] Past 100 steps: Average Loss 1.142 | Accuracy: 91%\n[Step 2800] Past 100 steps: Average Loss 1.017 | Accuracy: 88%\n[Step 2900] Past 100 steps: Average Loss 0.935 | Accuracy: 90%\n[Step 3000] Past 100 steps: Average Loss 1.172 | Accuracy: 86%\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "[Step 3100] Past 100 steps: Average Loss 2.010 | Accuracy: 80%\n[Step 3200] Past 100 steps: Average Loss 1.566 | Accuracy: 77%\n[Step 3300] Past 100 steps: Average Loss 0.924 | Accuracy: 90%\n[Step 3400] Past 100 steps: Average Loss 0.758 | Accuracy: 90%\n[Step 3500] Past 100 steps: Average Loss 2.031 | Accuracy: 81%\n[Step 3600] Past 100 steps: Average Loss 1.070 | Accuracy: 86%\n[Step 3700] Past 100 steps: Average Loss 0.560 | Accuracy: 92%\n[Step 3800] Past 100 steps: Average Loss 1.595 | Accuracy: 80%\n[Step 3900] Past 100 steps: Average Loss 0.793 | Accuracy: 87%\n[Step 4000] Past 100 steps: Average Loss 1.517 | Accuracy: 85%\n[Step 4100] Past 100 steps: Average Loss 0.716 | Accuracy: 90%\n[Step 4200] Past 100 steps: Average Loss 1.326 | Accuracy: 85%\n[Step 4300] Past 100 steps: Average Loss 0.633 | Accuracy: 91%\n[Step 4400] Past 100 steps: Average Loss 1.085 | Accuracy: 87%\n[Step 4500] Past 100 steps: Average Loss 1.230 | Accuracy: 83%\n[Step 4600] Past 100 steps: Average Loss 0.906 | Accuracy: 86%\n[Step 4700] Past 100 steps: Average Loss 0.897 | Accuracy: 85%\n[Step 4800] Past 100 steps: Average Loss 1.452 | Accuracy: 91%\n[Step 4900] Past 100 steps: Average Loss 0.966 | Accuracy: 89%\n[Step 5000] Past 100 steps: Average Loss 1.125 | Accuracy: 85%\n[Step 5100] Past 100 steps: Average Loss 0.939 | Accuracy: 87%\n[Step 5200] Past 100 steps: Average Loss 0.577 | Accuracy: 91%\n[Step 5300] Past 100 steps: Average Loss 1.169 | Accuracy: 82%\n[Step 5400] Past 100 steps: Average Loss 0.908 | Accuracy: 88%\n[Step 5500] Past 100 steps: Average Loss 0.920 | Accuracy: 87%\n[Step 5600] Past 100 steps: Average Loss 1.703 | Accuracy: 81%\n[Step 5700] Past 100 steps: Average Loss 1.247 | Accuracy: 85%\n[Step 5800] Past 100 steps: Average Loss 1.231 | Accuracy: 82%\n[Step 5900] Past 100 steps: Average Loss 1.414 | Accuracy: 85%\n[Step 6000] Past 100 steps: Average Loss 0.747 | Accuracy: 87%\n[Step 6100] Past 100 steps: Average Loss 0.958 | Accuracy: 88%\n[Step 6200] Past 100 steps: Average Loss 0.927 | Accuracy: 90%\n[Step 6300] Past 100 steps: Average Loss 1.390 | Accuracy: 83%\n[Step 6400] Past 100 steps: Average Loss 1.572 | Accuracy: 83%\n[Step 6500] Past 100 steps: Average Loss 0.865 | Accuracy: 85%\n[Step 6600] Past 100 steps: Average Loss 0.563 | Accuracy: 90%\n[Step 6700] Past 100 steps: Average Loss 0.990 | Accuracy: 86%\n[Step 6800] Past 100 steps: Average Loss 0.605 | Accuracy: 90%\n[Step 6900] Past 100 steps: Average Loss 0.782 | Accuracy: 89%\n[Step 7000] Past 100 steps: Average Loss 1.521 | Accuracy: 83%\n[Step 7100] Past 100 steps: Average Loss 1.298 | Accuracy: 86%\n[Step 7200] Past 100 steps: Average Loss 0.588 | Accuracy: 91%\n[Step 7300] Past 100 steps: Average Loss 0.845 | Accuracy: 87%\n[Step 7400] Past 100 steps: Average Loss 0.670 | Accuracy: 88%\n[Step 7500] Past 100 steps: Average Loss 0.832 | Accuracy: 88%\n[Step 7600] Past 100 steps: Average Loss 0.668 | Accuracy: 84%\n[Step 7700] Past 100 steps: Average Loss 0.830 | Accuracy: 94%\n[Step 7800] Past 100 steps: Average Loss 1.351 | Accuracy: 86%\n[Step 7900] Past 100 steps: Average Loss 1.003 | Accuracy: 84%\n[Step 8000] Past 100 steps: Average Loss 0.563 | Accuracy: 90%\n[Step 8100] Past 100 steps: Average Loss 0.948 | Accuracy: 87%\n[Step 8200] Past 100 steps: Average Loss 1.593 | Accuracy: 81%\n[Step 8300] Past 100 steps: Average Loss 1.240 | Accuracy: 86%\n[Step 8400] Past 100 steps: Average Loss 0.998 | Accuracy: 88%\n[Step 8500] Past 100 steps: Average Loss 0.816 | Accuracy: 89%\n[Step 8600] Past 100 steps: Average Loss 0.447 | Accuracy: 89%\n[Step 8700] Past 100 steps: Average Loss 1.255 | Accuracy: 86%\n[Step 8800] Past 100 steps: Average Loss 0.625 | Accuracy: 90%\n[Step 8900] Past 100 steps: Average Loss 1.126 | Accuracy: 90%\n[Step 9000] Past 100 steps: Average Loss 0.779 | Accuracy: 86%\n[Step 9100] Past 100 steps: Average Loss 0.630 | Accuracy: 92%\n[Step 9200] Past 100 steps: Average Loss 0.631 | Accuracy: 90%\n[Step 9300] Past 100 steps: Average Loss 1.349 | Accuracy: 86%\n[Step 9400] Past 100 steps: Average Loss 2.057 | Accuracy: 80%\n[Step 9500] Past 100 steps: Average Loss 0.538 | Accuracy: 89%\n[Step 9600] Past 100 steps: Average Loss 1.350 | Accuracy: 82%\n[Step 9700] Past 100 steps: Average Loss 1.684 | Accuracy: 87%\n[Step 9800] Past 100 steps: Average Loss 0.313 | Accuracy: 93%\n[Step 9900] Past 100 steps: Average Loss 0.618 | Accuracy: 90%\n[Step 10000] Past 100 steps: Average Loss 0.858 | Accuracy: 88%\n--- Epoch 3 ---\n[Step 100] Past 100 steps: Average Loss 1.097 | Accuracy: 84%\n[Step 200] Past 100 steps: Average Loss 0.509 | Accuracy: 92%\n[Step 300] Past 100 steps: Average Loss 0.894 | Accuracy: 87%\n[Step 400] Past 100 steps: Average Loss 0.322 | Accuracy: 94%\n[Step 500] Past 100 steps: Average Loss 0.311 | Accuracy: 92%\n[Step 600] Past 100 steps: Average Loss 0.599 | Accuracy: 90%\n[Step 700] Past 100 steps: Average Loss 0.766 | Accuracy: 88%\n[Step 800] Past 100 steps: Average Loss 0.338 | Accuracy: 94%\n[Step 900] Past 100 steps: Average Loss 0.391 | Accuracy: 91%\n[Step 1000] Past 100 steps: Average Loss 0.992 | Accuracy: 82%\n[Step 1100] Past 100 steps: Average Loss 0.405 | Accuracy: 92%\n[Step 1200] Past 100 steps: Average Loss 0.349 | Accuracy: 89%\n[Step 1300] Past 100 steps: Average Loss 0.250 | Accuracy: 93%\n[Step 1400] Past 100 steps: Average Loss 0.977 | Accuracy: 81%\n[Step 1500] Past 100 steps: Average Loss 0.596 | Accuracy: 86%\n[Step 1600] Past 100 steps: Average Loss 0.659 | Accuracy: 91%\n[Step 1700] Past 100 steps: Average Loss 0.485 | Accuracy: 89%\n[Step 1800] Past 100 steps: Average Loss 1.108 | Accuracy: 83%\n[Step 1900] Past 100 steps: Average Loss 1.371 | Accuracy: 80%\n[Step 2000] Past 100 steps: Average Loss 0.911 | Accuracy: 82%\n[Step 2100] Past 100 steps: Average Loss 1.034 | Accuracy: 93%\n[Step 2200] Past 100 steps: Average Loss 0.529 | Accuracy: 89%\n[Step 2300] Past 100 steps: Average Loss 0.890 | Accuracy: 88%\n[Step 2400] Past 100 steps: Average Loss 0.502 | Accuracy: 90%\n[Step 2500] Past 100 steps: Average Loss 0.415 | Accuracy: 91%\n[Step 2600] Past 100 steps: Average Loss 0.211 | Accuracy: 93%\n[Step 2700] Past 100 steps: Average Loss 0.746 | Accuracy: 89%\n[Step 2800] Past 100 steps: Average Loss 0.799 | Accuracy: 84%\n[Step 2900] Past 100 steps: Average Loss 0.572 | Accuracy: 92%\n[Step 3000] Past 100 steps: Average Loss 0.220 | Accuracy: 96%\n[Step 3100] Past 100 steps: Average Loss 0.838 | Accuracy: 91%\n[Step 3200] Past 100 steps: Average Loss 0.757 | Accuracy: 90%\n[Step 3300] Past 100 steps: Average Loss 0.846 | Accuracy: 87%\n[Step 3400] Past 100 steps: Average Loss 0.584 | Accuracy: 89%\n[Step 3500] Past 100 steps: Average Loss 0.669 | Accuracy: 89%\n[Step 3600] Past 100 steps: Average Loss 0.875 | Accuracy: 88%\n[Step 3700] Past 100 steps: Average Loss 0.332 | Accuracy: 92%\n[Step 3800] Past 100 steps: Average Loss 0.524 | Accuracy: 91%\n[Step 3900] Past 100 steps: Average Loss 1.336 | Accuracy: 83%\n[Step 4000] Past 100 steps: Average Loss 1.232 | Accuracy: 88%\n[Step 4100] Past 100 steps: Average Loss 0.260 | Accuracy: 95%\n[Step 4200] Past 100 steps: Average Loss 1.233 | Accuracy: 83%\n[Step 4300] Past 100 steps: Average Loss 0.710 | Accuracy: 88%\n[Step 4400] Past 100 steps: Average Loss 0.520 | Accuracy: 93%\n[Step 4500] Past 100 steps: Average Loss 0.338 | Accuracy: 87%\n[Step 4600] Past 100 steps: Average Loss 0.716 | Accuracy: 90%\n[Step 4700] Past 100 steps: Average Loss 0.726 | Accuracy: 86%\n[Step 4800] Past 100 steps: Average Loss 0.975 | Accuracy: 85%\n[Step 4900] Past 100 steps: Average Loss 0.874 | Accuracy: 89%\n[Step 5000] Past 100 steps: Average Loss 0.239 | Accuracy: 92%\n[Step 5100] Past 100 steps: Average Loss 0.919 | Accuracy: 89%\n[Step 5200] Past 100 steps: Average Loss 0.250 | Accuracy: 93%\n[Step 5300] Past 100 steps: Average Loss 0.510 | Accuracy: 89%\n[Step 5400] Past 100 steps: Average Loss 0.658 | Accuracy: 88%\n[Step 5500] Past 100 steps: Average Loss 0.331 | Accuracy: 91%\n[Step 5600] Past 100 steps: Average Loss 1.297 | Accuracy: 81%\n[Step 5700] Past 100 steps: Average Loss 0.504 | Accuracy: 91%\n[Step 5800] Past 100 steps: Average Loss 0.796 | Accuracy: 88%\n[Step 5900] Past 100 steps: Average Loss 0.553 | Accuracy: 89%\n[Step 6000] Past 100 steps: Average Loss 0.686 | Accuracy: 91%\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "[Step 6100] Past 100 steps: Average Loss 0.494 | Accuracy: 92%\n[Step 6200] Past 100 steps: Average Loss 0.852 | Accuracy: 85%\n[Step 6300] Past 100 steps: Average Loss 0.339 | Accuracy: 93%\n[Step 6400] Past 100 steps: Average Loss 0.536 | Accuracy: 90%\n[Step 6500] Past 100 steps: Average Loss 0.566 | Accuracy: 85%\n[Step 6600] Past 100 steps: Average Loss 0.835 | Accuracy: 86%\n[Step 6700] Past 100 steps: Average Loss 0.429 | Accuracy: 91%\n[Step 6800] Past 100 steps: Average Loss 0.575 | Accuracy: 88%\n[Step 6900] Past 100 steps: Average Loss 0.532 | Accuracy: 90%\n[Step 7000] Past 100 steps: Average Loss 0.724 | Accuracy: 84%\n[Step 7100] Past 100 steps: Average Loss 0.395 | Accuracy: 90%\n[Step 7200] Past 100 steps: Average Loss 0.310 | Accuracy: 92%\n[Step 7300] Past 100 steps: Average Loss 0.802 | Accuracy: 88%\n[Step 7400] Past 100 steps: Average Loss 1.035 | Accuracy: 84%\n[Step 7500] Past 100 steps: Average Loss 0.403 | Accuracy: 88%\n[Step 7600] Past 100 steps: Average Loss 0.505 | Accuracy: 84%\n[Step 7700] Past 100 steps: Average Loss 0.910 | Accuracy: 87%\n[Step 7800] Past 100 steps: Average Loss 1.021 | Accuracy: 87%\n[Step 7900] Past 100 steps: Average Loss 1.094 | Accuracy: 85%\n[Step 8000] Past 100 steps: Average Loss 0.667 | Accuracy: 86%\n[Step 8100] Past 100 steps: Average Loss 0.743 | Accuracy: 87%\n[Step 8200] Past 100 steps: Average Loss 0.722 | Accuracy: 86%\n[Step 8300] Past 100 steps: Average Loss 0.711 | Accuracy: 84%\n[Step 8400] Past 100 steps: Average Loss 0.427 | Accuracy: 88%\n[Step 8500] Past 100 steps: Average Loss 0.569 | Accuracy: 89%\n[Step 8600] Past 100 steps: Average Loss 0.573 | Accuracy: 88%\n[Step 8700] Past 100 steps: Average Loss 0.275 | Accuracy: 94%\n[Step 8800] Past 100 steps: Average Loss 0.436 | Accuracy: 93%\n[Step 8900] Past 100 steps: Average Loss 1.004 | Accuracy: 80%\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Now Using Keras"
        },
        {
            "metadata": {},
            "id": "311afbd3-8c02-4a68-95b6-44ffa4f21d84",
            "cell_type": "code",
            "source": "import numpy as np\nimport mnist\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\nfrom tensorflow.keras.utils import to_categorical\n\ntrain_images = mnist.train_images()\ntrain_labels = mnist.train_labels()\ntest_images=mnist.test_images()\ntest_labels=mnist.test_labels()\n\n#Normalising the images\ntest_images=(test_images/255)-0.5\ntrain_images=(train_images/255)-0.5\n\n#Reshaping the each images from 28*28 to 28*28*1\n#expand_dims converts axis 3 [0,0,0...] to [[0],[0],[0]...]\ntrain_images=np.expand_dims(train_images,axis=3)\ntest_images=np.expand_dims(test_images,axis=3)\n\nnum_filters = 8\nfilter_size = 3\npool_size = 2\n\n###1: Building model\n\n#we choose Sequential model\n#Conv2D > 28*28 to 26*26*8\n#Maxpooling > 26*26*8 to 13*13*8\n#flatten + Softmax as activation function > 13*13*8 to 1352 to 10\nmodel=Sequential([\n    Conv2D(num_filters,filter_size,input_shape=(28,28,1)),\n    MaxPooling2D(pool_size=pool_size),\n    Flatten(),\n    Dense(10, activation=\"softmax\"),\n])\n\n###2: configuring training process\n\n#optimizer: adam based\n#loss function: c_c\n#metrics: what we want*\nmodel.compile(\n    'adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n)\n\n###3: Training model\n\n# *we need[0,0,0,4,0,0,0,0,0,0] instead of [4] from labels\nmodel.fit(\n    train_images,\n    to_categorical(train_labels),\n    epochs=10,\n    validation_data=(test_images, to_categorical(test_labels)),\n)\n\n### 4: Saving weights\n\nmodel.save_weights('cnn.h5')\n\npredictions = model.predict(test_images[:5])\nprint(np.argmax(predictions, axis=1))\nprint(test_labels[:5])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Separation"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport mnist\n\n###***\ntest_images=mnist.test_images()\ntest_images=np.expand_dims((test_images/255)-0.5,axis=3)\ntest_labels=mnist.test_labels()\n\n### we skipped part 3 as we exported the weights\n\nnum_filters=8\nfilter_size=3\npool_size=2\n\nmodel=Sequential([\n    Conv2D(num_filters,filter_size,input_shape=(28,28,1)),\n    MaxPooling2D(pool_size=pool_size),\n    Flatten(),\n    Dense(10,activation='softmax'),\n])\n\nn=10000\nmodel.load_weights('cnn.h5')\nprediction = model.predict(test_images[:n])\nmax_pred=np.argmax(prediction,axis=1)\n\nDIFF=(max_pred)-test_labels[:n]\nwin=len(DIFF[DIFF==0])\nprint(\"total_acuracy=\",(100*win)/n,\"%\")\n\nfor i in np.random.randint(1,6000,20):\n    plt.imshow(test_images[i])\n    print(np.argmax(prediction,axis=1)[i],\"\\n\\n\\n\")\n    plt.show()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}