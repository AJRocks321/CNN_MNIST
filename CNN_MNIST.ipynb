{
    "cells": [
        {
            "metadata": {},
            "id": "223a24a4-a370-4b00-8470-36c037308173",
            "cell_type": "code",
            "source": "# Conv.py takes input number of filters and its function forward takes the image array as input and \n# overall returns 3d array with filters applied,\n# input : 8   28*28 array\n# Output: 26*26*8 array\nimport os\nos.system('pip install mnist')\nimport numpy as np\nclass conv3x3:\n    def __init__(self,num_filters):\n        self.num_filters=num_filters\n        self.filters=np.random.rand(num_filters,3,3)\n\n    def iterate_regions(self,image):\n        d1,d2=image.shape\n        for i in range(d1-2):\n            for j in range(d2-2):\n                region=image[i:(i+3),j:(j+3)]\n                yield region,i,j\n\n    def forward(self,input):\n        d1,d2=input.shape\n        self.last_input=input\n        output=np.zeros((d1-2,d2-2,self.num_filters))\n        for region,i,j in self.iterate_regions(input):\n            output[i,j]=np.sum(region*self.filters,axis=(1,2))\n        return output\n    \n    def backprop(self, d_L_d_out, learn_rate):\n\n        d_L_d_filters = np.zeros(self.filters.shape)\n\n        for im_region, i, j in self.iterate_regions(self.last_input):\n            for f in range(self.num_filters):\n                d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n\n        # Update filters\n        self.filters -= learn_rate * d_L_d_filters\n\n        return None",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {},
            "id": "39666f49-0d48-426b-bd09-84b901dd2c57",
            "cell_type": "code",
            "source": "#Maxpool's function forward takes 26*26*8 input and do maxpool and return 13*13*8 array.\nimport numpy as np\nclass max_pool:\n    def iterate_regions(self,image):\n        d1,d2,d3=image.shape\n        for i in range(d1//2):\n            for j in range(d2//2):\n                region=image[2*i:2*(i+1),2*j:2*(j+1)]\n                yield region,i,j\n    def forward(self,input):\n        d1,d2,d3=input.shape\n        self.last_input = input\n        output=np.zeros((d1//2,d2//2,d3))\n        for region,i,j in self.iterate_regions(input):\n            output[i,j]=np.max(region,axis=(0,1))\n        return output\n    def backprop(self, d_L_d_out):\n        d_L_d_input = np.zeros(self.last_input.shape)\n\n        for im_region, i, j in self.iterate_regions(self.last_input):\n            h, w, f = im_region.shape\n            amax = np.amax(im_region, axis=(0, 1))\n\n        for i2 in range(h):\n            for j2 in range(w):\n                for f2 in range(f):\n                    # If this pixel was the max value, copy the gradient to it.\n                    if im_region[i2, j2, f2] == amax[f2]:\n                        d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n\n        return d_L_d_input",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "id": "73deafc8-fb3c-4bbb-af94-f0ed3ab481fc",
            "cell_type": "code",
            "source": "# This class soft_max takes input of (number of elements in new maxpooled img \"13*13*8=1352\") \n# and (number of nodes 10) and forward function takes input of the img array\n# and returns a array 10 with probablities for each one.\nimport numpy as np\nclass soft_max:\n    def __init__(self,input_len,num_nodes):\n        self.weights=np.random.randn(input_len,num_nodes)/input_len\n        self.biases=np.zeros(num_nodes)\n    def forward(self,input):\n        self.last_input_shape = input.shape\n        input=input.flatten()\n        self.last_input = input\n        #calculaing wX+b\n        totals=np.dot(input,self.weights)+self.biases\n        self.last_totals=totals\n        #final output\n        expx=np.exp(totals)\n        output=expx/np.sum(expx,axis=0)\n        return output\n    def backprop(self,d_L_d_out,learn_rate):\n        for i, gradient in enumerate(d_L_d_out):\n            if gradient == 0:\n                continue\n\n            # e^totals\n            t_exp = np.exp(self.last_totals)\n\n            # Sum of all e^totals\n            S = np.sum(t_exp)\n\n            # Gradients of out[i] against totals\n            d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n            d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n\n            # Gradients of totals against weights/biases/input\n            d_t_d_w = self.last_input\n            d_t_d_b = 1\n            d_t_d_inputs = self.weights\n\n            # Gradients of loss against totals\n            d_L_d_t = gradient * d_out_d_t\n\n            # Gradients of loss against weights/biases/input\n            d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n            d_L_d_b = d_L_d_t * d_t_d_b\n            d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n\n            # Update weights / biases\n            self.weights -= learn_rate * d_L_d_w\n            self.biases -= learn_rate * d_L_d_b\n            return d_L_d_inputs.reshape(self.last_input_shape)\n",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {},
            "id": "327e01e7-3090-4db7-8f72-5487ec437665",
            "cell_type": "code",
            "source": "import mnist\nimport numpy as np\n\nconv=conv3x3(8)\npool=max_pool()\nSoftmax=soft_max(13*13*8,10)\n\ntrain_images=mnist.train_images()[:10000]\ntrain_labels=mnist.train_labels()[:10000]\ntest_images = mnist.test_images()[:1000]\ntest_labels = mnist.test_labels()[:1000]\n\ndef forward(image,label):\n    output=conv.forward((image/255)-0.5)\n    output=pool.forward(output)\n    output=Softmax.forward(output)\n\n    loss=-np.log(output[label])\n    accuracy=1 if np.argmax(output)==label else 0\n\n    return output, loss, accuracy\n\ndef train(im, label, lr=.005):\n        # Forward\n    out, loss, acc = forward(im, label)\n\n    gradient = np.zeros(10)\n    gradient[label] = -1 / out[label]\n\n    # Backprop\n    gradient = soft_max.backprop(Softmax,gradient, lr)\n    gradient = pool.backprop(gradient)\n    gradient = conv.backprop(gradient, lr)\n\n    return loss, acc\n\n\nprint('CNN initialised')\n\n# Training the CNN for 3 epochs\nfor epoch in range(3):\n    print('--- Epoch %d ---' % (epoch + 1))\n\n\n    permutation = np.random.permutation(len(train_images))\n    train_images = train_images[permutation]\n    train_labels = train_labels[permutation]\n\n    loss = 0\n    num_correct = 0\n    for i, (im, label) in enumerate(zip(train_images, train_labels)):\n        if i > 0 and i % 100 == 99:\n            print(\n                '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n                (i + 1, loss / 100, num_correct)\n            )\n            loss = 0\n            num_correct = 0\n\n        l, acc = train(im, label)\n        loss += l\n        num_correct += acc\n\n\nprint('\\n--- Testing the CNN ---')\nloss = 0\nnum_correct = 0\nfor im, label in zip(test_images, test_labels):\n    _, l, acc = forward(im, label)\n    loss += l\n    num_correct += acc\n\nnum_tests = len(test_images)\nprint('Test Loss:', loss / num_tests)\nprint('Test Accuracy:', num_correct / num_tests)",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "CNN initialised\n--- Epoch 1 ---\n[Step 100] Past 100 steps: Average Loss 10.392 | Accuracy: 25%\n[Step 200] Past 100 steps: Average Loss 8.239 | Accuracy: 51%\n[Step 300] Past 100 steps: Average Loss 5.514 | Accuracy: 59%\n[Step 400] Past 100 steps: Average Loss 5.159 | Accuracy: 59%\n[Step 500] Past 100 steps: Average Loss 4.309 | Accuracy: 69%\n[Step 600] Past 100 steps: Average Loss 3.683 | Accuracy: 65%\n[Step 700] Past 100 steps: Average Loss 4.444 | Accuracy: 65%\n[Step 800] Past 100 steps: Average Loss 5.305 | Accuracy: 63%\n[Step 900] Past 100 steps: Average Loss 5.281 | Accuracy: 70%\n[Step 1000] Past 100 steps: Average Loss 2.194 | Accuracy: 79%\n[Step 1100] Past 100 steps: Average Loss 2.946 | Accuracy: 77%\n[Step 1200] Past 100 steps: Average Loss 3.438 | Accuracy: 74%\n[Step 1300] Past 100 steps: Average Loss 2.177 | Accuracy: 82%\n[Step 1400] Past 100 steps: Average Loss 3.186 | Accuracy: 75%\n[Step 1500] Past 100 steps: Average Loss 4.102 | Accuracy: 73%\n[Step 1600] Past 100 steps: Average Loss 4.705 | Accuracy: 68%\n[Step 1700] Past 100 steps: Average Loss 3.049 | Accuracy: 77%\n[Step 1800] Past 100 steps: Average Loss 4.175 | Accuracy: 72%\n[Step 1900] Past 100 steps: Average Loss 2.763 | Accuracy: 77%\n[Step 2000] Past 100 steps: Average Loss 3.040 | Accuracy: 78%\n[Step 2100] Past 100 steps: Average Loss 3.280 | Accuracy: 74%\n[Step 2200] Past 100 steps: Average Loss 1.787 | Accuracy: 86%\n[Step 2300] Past 100 steps: Average Loss 2.224 | Accuracy: 76%\n[Step 2400] Past 100 steps: Average Loss 2.171 | Accuracy: 79%\n[Step 2500] Past 100 steps: Average Loss 1.794 | Accuracy: 84%\n[Step 2600] Past 100 steps: Average Loss 4.713 | Accuracy: 69%\n[Step 2700] Past 100 steps: Average Loss 3.738 | Accuracy: 77%\n[Step 2800] Past 100 steps: Average Loss 1.767 | Accuracy: 80%\n[Step 2900] Past 100 steps: Average Loss 2.669 | Accuracy: 80%\n[Step 3000] Past 100 steps: Average Loss 1.753 | Accuracy: 86%\n[Step 3100] Past 100 steps: Average Loss 2.873 | Accuracy: 79%\n[Step 3200] Past 100 steps: Average Loss 2.418 | Accuracy: 82%\n[Step 3300] Past 100 steps: Average Loss 4.200 | Accuracy: 74%\n[Step 3400] Past 100 steps: Average Loss 2.671 | Accuracy: 80%\n[Step 3500] Past 100 steps: Average Loss 2.312 | Accuracy: 84%\n[Step 3600] Past 100 steps: Average Loss 2.372 | Accuracy: 88%\n[Step 3700] Past 100 steps: Average Loss 1.955 | Accuracy: 81%\n[Step 3800] Past 100 steps: Average Loss 4.013 | Accuracy: 72%\n[Step 3900] Past 100 steps: Average Loss 2.140 | Accuracy: 82%\n[Step 4000] Past 100 steps: Average Loss 2.767 | Accuracy: 82%\n[Step 4100] Past 100 steps: Average Loss 2.702 | Accuracy: 76%\n[Step 4200] Past 100 steps: Average Loss 3.694 | Accuracy: 75%\n[Step 4300] Past 100 steps: Average Loss 2.897 | Accuracy: 75%\n[Step 4400] Past 100 steps: Average Loss 1.579 | Accuracy: 87%\n[Step 4500] Past 100 steps: Average Loss 2.418 | Accuracy: 81%\n[Step 4600] Past 100 steps: Average Loss 2.903 | Accuracy: 78%\n[Step 4700] Past 100 steps: Average Loss 2.767 | Accuracy: 79%\n[Step 4800] Past 100 steps: Average Loss 1.989 | Accuracy: 81%\n[Step 4900] Past 100 steps: Average Loss 0.857 | Accuracy: 89%\n[Step 5000] Past 100 steps: Average Loss 2.375 | Accuracy: 82%\n[Step 5100] Past 100 steps: Average Loss 2.282 | Accuracy: 86%\n[Step 5200] Past 100 steps: Average Loss 2.409 | Accuracy: 79%\n[Step 5300] Past 100 steps: Average Loss 2.302 | Accuracy: 79%\n[Step 5400] Past 100 steps: Average Loss 2.036 | Accuracy: 82%\n[Step 5500] Past 100 steps: Average Loss 1.369 | Accuracy: 89%\n[Step 5600] Past 100 steps: Average Loss 1.443 | Accuracy: 85%\n[Step 5700] Past 100 steps: Average Loss 1.557 | Accuracy: 85%\n[Step 5800] Past 100 steps: Average Loss 0.866 | Accuracy: 88%\n[Step 5900] Past 100 steps: Average Loss 2.593 | Accuracy: 82%\n[Step 6000] Past 100 steps: Average Loss 1.877 | Accuracy: 85%\n[Step 6100] Past 100 steps: Average Loss 1.376 | Accuracy: 85%\n[Step 6200] Past 100 steps: Average Loss 2.056 | Accuracy: 81%\n[Step 6300] Past 100 steps: Average Loss 1.231 | Accuracy: 86%\n[Step 6400] Past 100 steps: Average Loss 2.309 | Accuracy: 84%\n[Step 6500] Past 100 steps: Average Loss 1.958 | Accuracy: 85%\n[Step 6600] Past 100 steps: Average Loss 2.188 | Accuracy: 79%\n[Step 6700] Past 100 steps: Average Loss 2.081 | Accuracy: 81%\n[Step 6800] Past 100 steps: Average Loss 1.332 | Accuracy: 86%\n[Step 6900] Past 100 steps: Average Loss 2.006 | Accuracy: 85%\n[Step 7000] Past 100 steps: Average Loss 1.485 | Accuracy: 87%\n[Step 7100] Past 100 steps: Average Loss 1.034 | Accuracy: 90%\n[Step 7200] Past 100 steps: Average Loss 2.991 | Accuracy: 79%\n[Step 7300] Past 100 steps: Average Loss 1.435 | Accuracy: 87%\n[Step 7400] Past 100 steps: Average Loss 2.696 | Accuracy: 82%\n[Step 7500] Past 100 steps: Average Loss 1.565 | Accuracy: 83%\n[Step 7600] Past 100 steps: Average Loss 2.507 | Accuracy: 80%\n[Step 7700] Past 100 steps: Average Loss 1.416 | Accuracy: 86%\n[Step 7800] Past 100 steps: Average Loss 3.908 | Accuracy: 80%\n[Step 7900] Past 100 steps: Average Loss 1.378 | Accuracy: 83%\n[Step 8000] Past 100 steps: Average Loss 2.005 | Accuracy: 83%\n[Step 8100] Past 100 steps: Average Loss 1.396 | Accuracy: 85%\n[Step 8200] Past 100 steps: Average Loss 1.544 | Accuracy: 84%\n[Step 8300] Past 100 steps: Average Loss 2.556 | Accuracy: 79%\n[Step 8400] Past 100 steps: Average Loss 2.485 | Accuracy: 85%\n[Step 8500] Past 100 steps: Average Loss 0.838 | Accuracy: 91%\n[Step 8600] Past 100 steps: Average Loss 1.348 | Accuracy: 85%\n[Step 8700] Past 100 steps: Average Loss 2.993 | Accuracy: 78%\n[Step 8800] Past 100 steps: Average Loss 2.004 | Accuracy: 79%\n[Step 8900] Past 100 steps: Average Loss 1.844 | Accuracy: 83%\n[Step 9000] Past 100 steps: Average Loss 0.789 | Accuracy: 92%\n[Step 9100] Past 100 steps: Average Loss 1.859 | Accuracy: 84%\n[Step 9200] Past 100 steps: Average Loss 0.864 | Accuracy: 87%\n[Step 9300] Past 100 steps: Average Loss 1.741 | Accuracy: 82%\n[Step 9400] Past 100 steps: Average Loss 2.543 | Accuracy: 78%\n[Step 9500] Past 100 steps: Average Loss 1.296 | Accuracy: 82%\n[Step 9600] Past 100 steps: Average Loss 1.176 | Accuracy: 88%\n[Step 9700] Past 100 steps: Average Loss 1.457 | Accuracy: 84%\n[Step 9800] Past 100 steps: Average Loss 1.282 | Accuracy: 83%\n[Step 9900] Past 100 steps: Average Loss 1.265 | Accuracy: 86%\n[Step 10000] Past 100 steps: Average Loss 1.164 | Accuracy: 91%\n--- Epoch 2 ---\n[Step 100] Past 100 steps: Average Loss 1.821 | Accuracy: 85%\n[Step 200] Past 100 steps: Average Loss 1.350 | Accuracy: 84%\n[Step 300] Past 100 steps: Average Loss 1.720 | Accuracy: 81%\n[Step 400] Past 100 steps: Average Loss 1.265 | Accuracy: 82%\n[Step 500] Past 100 steps: Average Loss 0.738 | Accuracy: 84%\n[Step 600] Past 100 steps: Average Loss 0.767 | Accuracy: 90%\n[Step 700] Past 100 steps: Average Loss 1.581 | Accuracy: 86%\n[Step 800] Past 100 steps: Average Loss 0.746 | Accuracy: 91%\n[Step 900] Past 100 steps: Average Loss 2.091 | Accuracy: 83%\n[Step 1000] Past 100 steps: Average Loss 0.625 | Accuracy: 89%\n[Step 1100] Past 100 steps: Average Loss 0.941 | Accuracy: 88%\n[Step 1200] Past 100 steps: Average Loss 1.975 | Accuracy: 83%\n[Step 1300] Past 100 steps: Average Loss 1.375 | Accuracy: 85%\n[Step 1400] Past 100 steps: Average Loss 1.158 | Accuracy: 84%\n[Step 1500] Past 100 steps: Average Loss 1.352 | Accuracy: 86%\n[Step 1600] Past 100 steps: Average Loss 1.044 | Accuracy: 86%\n[Step 1700] Past 100 steps: Average Loss 2.474 | Accuracy: 83%\n[Step 1800] Past 100 steps: Average Loss 1.184 | Accuracy: 86%\n[Step 1900] Past 100 steps: Average Loss 0.616 | Accuracy: 90%\n[Step 2000] Past 100 steps: Average Loss 0.955 | Accuracy: 89%\n[Step 2100] Past 100 steps: Average Loss 1.317 | Accuracy: 87%\n[Step 2200] Past 100 steps: Average Loss 0.483 | Accuracy: 92%\n[Step 2300] Past 100 steps: Average Loss 1.035 | Accuracy: 90%\n[Step 2400] Past 100 steps: Average Loss 1.743 | Accuracy: 84%\n[Step 2500] Past 100 steps: Average Loss 1.508 | Accuracy: 87%\n[Step 2600] Past 100 steps: Average Loss 2.195 | Accuracy: 85%\n[Step 2700] Past 100 steps: Average Loss 1.502 | Accuracy: 89%\n[Step 2800] Past 100 steps: Average Loss 0.716 | Accuracy: 91%\n[Step 2900] Past 100 steps: Average Loss 0.786 | Accuracy: 93%\n[Step 3000] Past 100 steps: Average Loss 1.050 | Accuracy: 88%\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "[Step 3100] Past 100 steps: Average Loss 1.292 | Accuracy: 86%\n[Step 3200] Past 100 steps: Average Loss 0.955 | Accuracy: 85%\n[Step 3300] Past 100 steps: Average Loss 0.713 | Accuracy: 85%\n[Step 3400] Past 100 steps: Average Loss 1.239 | Accuracy: 84%\n[Step 3500] Past 100 steps: Average Loss 1.040 | Accuracy: 85%\n[Step 3600] Past 100 steps: Average Loss 0.971 | Accuracy: 91%\n[Step 3700] Past 100 steps: Average Loss 0.740 | Accuracy: 86%\n[Step 3800] Past 100 steps: Average Loss 2.441 | Accuracy: 81%\n[Step 3900] Past 100 steps: Average Loss 0.575 | Accuracy: 92%\n[Step 4000] Past 100 steps: Average Loss 0.659 | Accuracy: 90%\n[Step 4100] Past 100 steps: Average Loss 0.703 | Accuracy: 88%\n[Step 4200] Past 100 steps: Average Loss 2.003 | Accuracy: 78%\n[Step 4300] Past 100 steps: Average Loss 0.618 | Accuracy: 92%\n[Step 4400] Past 100 steps: Average Loss 0.967 | Accuracy: 85%\n[Step 4500] Past 100 steps: Average Loss 0.632 | Accuracy: 89%\n[Step 4600] Past 100 steps: Average Loss 0.742 | Accuracy: 91%\n[Step 4700] Past 100 steps: Average Loss 1.559 | Accuracy: 82%\n[Step 4800] Past 100 steps: Average Loss 1.830 | Accuracy: 81%\n[Step 4900] Past 100 steps: Average Loss 1.716 | Accuracy: 78%\n[Step 5000] Past 100 steps: Average Loss 1.132 | Accuracy: 87%\n[Step 5100] Past 100 steps: Average Loss 0.685 | Accuracy: 94%\n[Step 5200] Past 100 steps: Average Loss 1.093 | Accuracy: 88%\n[Step 5300] Past 100 steps: Average Loss 1.365 | Accuracy: 88%\n[Step 5400] Past 100 steps: Average Loss 1.659 | Accuracy: 82%\n[Step 5500] Past 100 steps: Average Loss 1.071 | Accuracy: 90%\n[Step 5600] Past 100 steps: Average Loss 1.348 | Accuracy: 84%\n[Step 5700] Past 100 steps: Average Loss 0.830 | Accuracy: 87%\n[Step 5800] Past 100 steps: Average Loss 0.607 | Accuracy: 88%\n[Step 5900] Past 100 steps: Average Loss 0.976 | Accuracy: 88%\n[Step 6000] Past 100 steps: Average Loss 1.234 | Accuracy: 84%\n[Step 6100] Past 100 steps: Average Loss 1.479 | Accuracy: 84%\n[Step 6200] Past 100 steps: Average Loss 1.291 | Accuracy: 87%\n[Step 6300] Past 100 steps: Average Loss 0.594 | Accuracy: 88%\n[Step 6400] Past 100 steps: Average Loss 1.163 | Accuracy: 83%\n[Step 6500] Past 100 steps: Average Loss 1.864 | Accuracy: 85%\n[Step 6600] Past 100 steps: Average Loss 0.692 | Accuracy: 88%\n[Step 6700] Past 100 steps: Average Loss 0.946 | Accuracy: 88%\n[Step 6800] Past 100 steps: Average Loss 1.218 | Accuracy: 84%\n[Step 6900] Past 100 steps: Average Loss 0.931 | Accuracy: 89%\n[Step 7000] Past 100 steps: Average Loss 2.239 | Accuracy: 81%\n[Step 7100] Past 100 steps: Average Loss 0.925 | Accuracy: 86%\n[Step 7200] Past 100 steps: Average Loss 1.025 | Accuracy: 89%\n[Step 7300] Past 100 steps: Average Loss 0.448 | Accuracy: 92%\n[Step 7400] Past 100 steps: Average Loss 0.424 | Accuracy: 91%\n[Step 7500] Past 100 steps: Average Loss 0.441 | Accuracy: 94%\n[Step 7600] Past 100 steps: Average Loss 0.416 | Accuracy: 92%\n[Step 7700] Past 100 steps: Average Loss 1.215 | Accuracy: 87%\n[Step 7800] Past 100 steps: Average Loss 0.375 | Accuracy: 92%\n[Step 7900] Past 100 steps: Average Loss 0.781 | Accuracy: 91%\n[Step 8000] Past 100 steps: Average Loss 1.007 | Accuracy: 88%\n[Step 8100] Past 100 steps: Average Loss 0.893 | Accuracy: 85%\n[Step 8200] Past 100 steps: Average Loss 1.250 | Accuracy: 82%\n[Step 8300] Past 100 steps: Average Loss 0.455 | Accuracy: 96%\n[Step 8400] Past 100 steps: Average Loss 1.222 | Accuracy: 84%\n[Step 8500] Past 100 steps: Average Loss 0.903 | Accuracy: 90%\n[Step 8600] Past 100 steps: Average Loss 0.721 | Accuracy: 87%\n[Step 8700] Past 100 steps: Average Loss 0.439 | Accuracy: 91%\n[Step 8800] Past 100 steps: Average Loss 1.059 | Accuracy: 87%\n[Step 8900] Past 100 steps: Average Loss 0.413 | Accuracy: 91%\n[Step 9000] Past 100 steps: Average Loss 0.794 | Accuracy: 89%\n[Step 9100] Past 100 steps: Average Loss 1.062 | Accuracy: 88%\n[Step 9200] Past 100 steps: Average Loss 0.654 | Accuracy: 87%\n[Step 9300] Past 100 steps: Average Loss 0.410 | Accuracy: 94%\n[Step 9400] Past 100 steps: Average Loss 0.693 | Accuracy: 89%\n[Step 9500] Past 100 steps: Average Loss 0.990 | Accuracy: 90%\n[Step 9600] Past 100 steps: Average Loss 1.105 | Accuracy: 88%\n[Step 9700] Past 100 steps: Average Loss 0.679 | Accuracy: 88%\n[Step 9800] Past 100 steps: Average Loss 0.823 | Accuracy: 85%\n[Step 9900] Past 100 steps: Average Loss 0.548 | Accuracy: 89%\n[Step 10000] Past 100 steps: Average Loss 1.096 | Accuracy: 89%\n--- Epoch 3 ---\n[Step 100] Past 100 steps: Average Loss 0.789 | Accuracy: 86%\n[Step 200] Past 100 steps: Average Loss 0.724 | Accuracy: 85%\n[Step 300] Past 100 steps: Average Loss 0.803 | Accuracy: 85%\n[Step 400] Past 100 steps: Average Loss 0.658 | Accuracy: 88%\n[Step 500] Past 100 steps: Average Loss 0.918 | Accuracy: 89%\n[Step 600] Past 100 steps: Average Loss 0.359 | Accuracy: 93%\n[Step 700] Past 100 steps: Average Loss 0.414 | Accuracy: 93%\n[Step 800] Past 100 steps: Average Loss 0.908 | Accuracy: 87%\n[Step 900] Past 100 steps: Average Loss 0.797 | Accuracy: 89%\n[Step 1000] Past 100 steps: Average Loss 0.650 | Accuracy: 90%\n[Step 1100] Past 100 steps: Average Loss 0.324 | Accuracy: 89%\n[Step 1200] Past 100 steps: Average Loss 0.822 | Accuracy: 91%\n[Step 1300] Past 100 steps: Average Loss 0.789 | Accuracy: 86%\n[Step 1400] Past 100 steps: Average Loss 1.068 | Accuracy: 88%\n[Step 1500] Past 100 steps: Average Loss 0.974 | Accuracy: 89%\n[Step 1600] Past 100 steps: Average Loss 0.601 | Accuracy: 88%\n[Step 1700] Past 100 steps: Average Loss 1.039 | Accuracy: 88%\n[Step 1800] Past 100 steps: Average Loss 1.264 | Accuracy: 84%\n[Step 1900] Past 100 steps: Average Loss 0.712 | Accuracy: 89%\n[Step 2000] Past 100 steps: Average Loss 1.000 | Accuracy: 86%\n[Step 2100] Past 100 steps: Average Loss 0.744 | Accuracy: 91%\n[Step 2200] Past 100 steps: Average Loss 0.821 | Accuracy: 85%\n[Step 2300] Past 100 steps: Average Loss 0.476 | Accuracy: 92%\n[Step 2400] Past 100 steps: Average Loss 0.571 | Accuracy: 89%\n[Step 2500] Past 100 steps: Average Loss 0.447 | Accuracy: 93%\n[Step 2600] Past 100 steps: Average Loss 0.187 | Accuracy: 94%\n[Step 2700] Past 100 steps: Average Loss 0.880 | Accuracy: 89%\n[Step 2800] Past 100 steps: Average Loss 1.443 | Accuracy: 86%\n[Step 2900] Past 100 steps: Average Loss 1.034 | Accuracy: 86%\n[Step 3000] Past 100 steps: Average Loss 0.995 | Accuracy: 89%\n[Step 3100] Past 100 steps: Average Loss 0.874 | Accuracy: 88%\n[Step 3200] Past 100 steps: Average Loss 0.579 | Accuracy: 87%\n[Step 3300] Past 100 steps: Average Loss 0.490 | Accuracy: 91%\n[Step 3400] Past 100 steps: Average Loss 0.986 | Accuracy: 87%\n[Step 3500] Past 100 steps: Average Loss 0.446 | Accuracy: 95%\n[Step 3600] Past 100 steps: Average Loss 0.197 | Accuracy: 95%\n[Step 3700] Past 100 steps: Average Loss 0.597 | Accuracy: 91%\n[Step 3800] Past 100 steps: Average Loss 0.295 | Accuracy: 92%\n[Step 3900] Past 100 steps: Average Loss 0.547 | Accuracy: 87%\n[Step 4000] Past 100 steps: Average Loss 0.492 | Accuracy: 86%\n[Step 4100] Past 100 steps: Average Loss 0.897 | Accuracy: 86%\n[Step 4200] Past 100 steps: Average Loss 0.574 | Accuracy: 92%\n[Step 4300] Past 100 steps: Average Loss 0.788 | Accuracy: 88%\n[Step 4400] Past 100 steps: Average Loss 0.252 | Accuracy: 95%\n[Step 4500] Past 100 steps: Average Loss 1.114 | Accuracy: 89%\n[Step 4600] Past 100 steps: Average Loss 0.736 | Accuracy: 90%\n[Step 4700] Past 100 steps: Average Loss 0.558 | Accuracy: 92%\n[Step 4800] Past 100 steps: Average Loss 0.549 | Accuracy: 92%\n[Step 4900] Past 100 steps: Average Loss 1.425 | Accuracy: 80%\n[Step 5000] Past 100 steps: Average Loss 0.480 | Accuracy: 92%\n[Step 5100] Past 100 steps: Average Loss 0.807 | Accuracy: 88%\n[Step 5200] Past 100 steps: Average Loss 0.638 | Accuracy: 87%\n[Step 5300] Past 100 steps: Average Loss 0.566 | Accuracy: 88%\n[Step 5400] Past 100 steps: Average Loss 0.701 | Accuracy: 87%\n[Step 5500] Past 100 steps: Average Loss 0.482 | Accuracy: 88%\n[Step 5600] Past 100 steps: Average Loss 0.357 | Accuracy: 93%\n[Step 5700] Past 100 steps: Average Loss 0.543 | Accuracy: 94%\n[Step 5800] Past 100 steps: Average Loss 0.501 | Accuracy: 92%\n[Step 5900] Past 100 steps: Average Loss 0.380 | Accuracy: 93%\n[Step 6000] Past 100 steps: Average Loss 0.247 | Accuracy: 91%\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "[Step 6100] Past 100 steps: Average Loss 0.585 | Accuracy: 89%\n[Step 6200] Past 100 steps: Average Loss 0.856 | Accuracy: 85%\n[Step 6300] Past 100 steps: Average Loss 0.727 | Accuracy: 86%\n[Step 6400] Past 100 steps: Average Loss 0.703 | Accuracy: 90%\n[Step 6500] Past 100 steps: Average Loss 0.698 | Accuracy: 90%\n[Step 6600] Past 100 steps: Average Loss 0.793 | Accuracy: 88%\n[Step 6700] Past 100 steps: Average Loss 0.714 | Accuracy: 86%\n[Step 6800] Past 100 steps: Average Loss 0.367 | Accuracy: 90%\n[Step 6900] Past 100 steps: Average Loss 1.127 | Accuracy: 83%\n[Step 7000] Past 100 steps: Average Loss 0.633 | Accuracy: 87%\n[Step 7100] Past 100 steps: Average Loss 0.270 | Accuracy: 94%\n[Step 7200] Past 100 steps: Average Loss 0.300 | Accuracy: 94%\n[Step 7300] Past 100 steps: Average Loss 0.546 | Accuracy: 92%\n[Step 7400] Past 100 steps: Average Loss 0.670 | Accuracy: 87%\n[Step 7500] Past 100 steps: Average Loss 0.339 | Accuracy: 91%\n[Step 7600] Past 100 steps: Average Loss 0.693 | Accuracy: 90%\n[Step 7700] Past 100 steps: Average Loss 0.570 | Accuracy: 91%\n[Step 7800] Past 100 steps: Average Loss 0.305 | Accuracy: 90%\n[Step 7900] Past 100 steps: Average Loss 0.779 | Accuracy: 88%\n[Step 8000] Past 100 steps: Average Loss 0.843 | Accuracy: 89%\n[Step 8100] Past 100 steps: Average Loss 0.587 | Accuracy: 89%\n[Step 8200] Past 100 steps: Average Loss 0.254 | Accuracy: 92%\n[Step 8300] Past 100 steps: Average Loss 0.906 | Accuracy: 85%\n[Step 8400] Past 100 steps: Average Loss 0.585 | Accuracy: 89%\n[Step 8500] Past 100 steps: Average Loss 0.631 | Accuracy: 91%\n[Step 8600] Past 100 steps: Average Loss 0.650 | Accuracy: 91%\n[Step 8700] Past 100 steps: Average Loss 0.685 | Accuracy: 89%\n[Step 8800] Past 100 steps: Average Loss 0.606 | Accuracy: 88%\n[Step 8900] Past 100 steps: Average Loss 0.369 | Accuracy: 90%\n[Step 9000] Past 100 steps: Average Loss 0.411 | Accuracy: 91%\n[Step 9100] Past 100 steps: Average Loss 0.338 | Accuracy: 93%\n[Step 9200] Past 100 steps: Average Loss 0.750 | Accuracy: 90%\n[Step 9300] Past 100 steps: Average Loss 0.886 | Accuracy: 87%\n[Step 9400] Past 100 steps: Average Loss 0.776 | Accuracy: 88%\n[Step 9500] Past 100 steps: Average Loss 0.655 | Accuracy: 88%\n[Step 9600] Past 100 steps: Average Loss 0.625 | Accuracy: 87%\n[Step 9700] Past 100 steps: Average Loss 0.568 | Accuracy: 89%\n[Step 9800] Past 100 steps: Average Loss 0.892 | Accuracy: 85%\n[Step 9900] Past 100 steps: Average Loss 0.481 | Accuracy: 92%\n[Step 10000] Past 100 steps: Average Loss 0.456 | Accuracy: 91%\n\n--- Testing the CNN ---\nTest Loss: 0.6096605861685224\nTest Accuracy: 0.875\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "id": "efca0323-128d-4043-a28f-0c4a2a13dbe7",
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "id": "311afbd3-8c02-4a68-95b6-44ffa4f21d84",
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}